{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12016776,"sourceType":"datasetVersion","datasetId":7560222},{"sourceId":12017703,"sourceType":"datasetVersion","datasetId":7560839},{"sourceId":12019208,"sourceType":"datasetVersion","datasetId":7561855}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§  Critical Question Generation using Fine-Tuned Flan-T5-small\nThis notebook presents an end-to-end pipeline for generating critical questions using a fine-tuned Flan-T5-small model. Critical Question Generation (CQG) involves generating insightful, probing questions from a given context and answer. The project showcases data processing, model fine-tuning, and evaluation using state-of-the-art open-source tools.","metadata":{}},{"cell_type":"markdown","source":"## 1. Installing Project Dependencies\nThis cell installs all the core dependencies required for the critical question generation project that uses the FLAN-T5-small model. Each library serves a distinct role in enabling model usage, data handling, evaluation, and natural language processing.","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.51.3 datasets evaluate sentencepiece nltk bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:59:29.445825Z","iopub.execute_input":"2025-06-01T14:59:29.446152Z","iopub.status.idle":"2025-06-01T14:59:33.127458Z","shell.execute_reply.started":"2025-06-01T14:59:29.446128Z","shell.execute_reply":"2025-06-01T14:59:33.126657Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.51.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.51.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.51.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.51.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.51.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.51.3) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.51.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.51.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.51.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.51.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.51.3) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Suppressing Warning Messages\n\nThis cell is used to suppress warning messages that may be generated during the execution of the notebook. The `warnings` module is imported, and the `filterwarnings('ignore')` function is called to prevent warnings from being displayed in the output. Suppressing warnings can help produce cleaner notebook outputs by hiding non-critical warnings that might otherwise clutter the results. ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:59:33.129257Z","iopub.execute_input":"2025-06-01T14:59:33.130168Z","iopub.status.idle":"2025-06-01T14:59:33.133851Z","shell.execute_reply.started":"2025-06-01T14:59:33.130138Z","shell.execute_reply":"2025-06-01T14:59:33.133250Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 3. Data Preparation and Paraphrasing for Critical Question Generation\n\nThis code cell performs several key tasks in preparing training data for generating critical questions using the FLAN-T5 model, with an emphasis on paraphrasing and data augmentation. The process is organized as follows:\n\n### 3a. Importing Required Libraries\n\nThe code imports necessary libraries for file handling, data processing, text similarity evaluation, deep learning, and model loading:\n- `os`, `json`, `time`, `random`: Standard Python utilities for system operations and data handling.\n- `pandas`: For tabular data processing and exporting to CSV.\n- `SequenceMatcher` from `difflib`: Used to evaluate similarity between original and paraphrased questions.\n- `torch`: For device management (CPU/GPU) and tensor operations.\n- `transformers`: To load the pre-trained T5 model and tokenizer.\n\n### 3b. Configuration\n\nSeveral configuration parameters are defined:\n- `DATA_FILE`: Path to the input JSON file containing validation data.\n- `MODEL_NAME`: Specifies the pre-trained paraphrasing model (T5-based) to be used.\n- `PARAPHRASES_PER_CQ`: Number of paraphrases to generate for each critical question.\n- `MAX_ENTRIES`: Allows limiting the number of entries for processing (set to `None` to process all entries).\n\n### 3c. Model and Tokenizer Loading\n\nThe code automatically detects whether a GPU is available and loads the specified model and tokenizer accordingly, ensuring efficient computation.\n\n### 3d. Paraphrase Quality Function\n\nThe `is_good_paraphrase` function checks whether a generated paraphrase is sufficiently different from the original question and that its length exceeds a threshold. This helps filter out trivial or uninformative paraphrases.\n\n### 3e. Batch Paraphrase Generation\n\nThe `batch_paraphrase` function uses the loaded model to generate paraphrases for a list of questions. It:\n- Prepares prompts in the appropriate format for the paraphrasing model.\n- Tokenizes and encodes the input batch.\n- Generates paraphrases using beam search for diversity.\n- Decodes outputs and filters them using the `is_good_paraphrase` function.\n- Returns a grouped list of filtered paraphrases for each input question.\n\n### 3f. Data Loading and Preparation\n\nThe `load_and_prepare` function reads the input data, processes each entry, and structures the data for model training:\n- Loads the dataset from the specified JSON file.\n- Extracts arguments, argumentation schemes, and critical questions labeled as \"useful\".\n- Groups critical questions into sets of three, as required for the generation task.\n- Constructs input prompts and target outputs for each group, following a consistent format suitable for training sequence-to-sequence models.\n- For each group of critical questions, generates paraphrases (if enabled) and creates additional data entries with paraphrased questions, enhancing the diversity of the training dataset.\n\n### 3g. Data Export\n\nThe processed data is saved in both CSV and JSON formats for flexibility in downstream tasks and reproducibility.\n\n### 3h. Output\n\nThe script prints the total number of examples generated, including both original and paraphrased entries.\nThis cell provides a comprehensive pipeline for augmenting and structuring critical question generation data, facilitating robust model training with enhanced linguistic variety.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport random\nimport pandas as pd\nfrom difflib import SequenceMatcher\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# --------------------- CONFIG ---------------------\nDATA_FILE = \"/kaggle/input/training-flant5/validation.json\"\nMODEL_NAME = \"Vamsi/T5_Paraphrase_Paws\"\nPARAPHRASES_PER_CQ = 1\nMAX_ENTRIES = None\n# --------------------------------------------------\n\n# Load model and tokenizer\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n\ndef is_good_paraphrase(original, paraphrase):\n    \"\"\"Check if the paraphrase is different enough and not too short.\"\"\"\n    ratio = SequenceMatcher(None, original.lower(), paraphrase.lower()).ratio()\n    return ratio < 0.85 and len(paraphrase.split()) > 4\n\ndef batch_paraphrase(questions, num_return_sequences=1, max_length=128):\n    \"\"\"Generate paraphrases for a list of questions.\"\"\"\n    prompts = [f\"paraphrase: {q} </s>\" for q in questions]\n    encoding = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n\n    outputs = model.generate(\n        input_ids=encoding[\"input_ids\"],\n        attention_mask=encoding[\"attention_mask\"],\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        num_beams=max(4, num_return_sequences),\n        early_stopping=True\n    )\n\n    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    # Group decoded outputs per input question\n    grouped = [decoded[i*num_return_sequences:(i+1)*num_return_sequences] for i in range(len(questions))]\n\n    clean_results = []\n    for orig, paras in zip(questions, grouped):\n        filtered = [p for p in paras if is_good_paraphrase(orig, p)]\n        clean_results.append(filtered)\n    return clean_results\n\ndef load_and_prepare(file_path, paraphrase_each_cq=True, paraphrases_per_cq=1, max_entries=None):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} does not exist.\")\n    with open(file_path, \"r\") as f:\n        raw_data = json.load(f)\n\n    data = []\n    entries = list(raw_data.items())[:max_entries] if max_entries else list(raw_data.items())\n\n    for k, entry in entries:\n        argument = entry[\"intervention\"].replace(\"\\n\", \" \").strip()\n        schemes = list(set(entry.get(\"schemes\", [])))\n        scheme_str = \", \".join(schemes)\n\n        useful_cqs = [\n            cq[\"cq\"].strip()\n            for cq in entry.get(\"cqs\", [])\n            if cq.get(\"label\", \"\").lower() == \"useful\"\n        ]\n\n        for i in range(0, len(useful_cqs), 3):\n            cq_group = useful_cqs[i:i+3]\n            if len(cq_group) == 3:\n                input_text = (\n                    \"Given the following argumentation scheme and argument, generate exactly three distinct and critical questions. \"\n                    \"Each question should challenge the argument from a different angle. Present the questions in a numbered list format, \"\n                    \"starting from 1: Scheme: {scheme}. Argument: {argument} 1. 2. 3.\"\n                ).format(scheme=scheme_str, argument=argument)\n                target_text = f\"1. {cq_group[0]} 2. {cq_group[1]} 3. {cq_group[2]}\"\n                data.append({\n                    \"input_text\": input_text,\n                    \"target_text\": target_text,\n                    \"paraphrased_from\": None\n                })\n\n                if paraphrase_each_cq:\n                    paraphrases_batch = batch_paraphrase(cq_group, num_return_sequences=paraphrases_per_cq)\n\n                    for idx, paraphrase_list in enumerate(paraphrases_batch):\n                        for para in paraphrase_list:\n                            new_group = cq_group.copy()\n                            new_group[idx] = para\n                            new_target = f\"1. {new_group[0]} 2. {new_group[1]} 3. {new_group[2]}\"\n                            data.append({\n                                \"input_text\": input_text,\n                                \"target_text\": new_target,\n                                \"paraphrased_from\": cq_group[idx]\n                            })\n    return data\n\n# ---------------- RUN ------------------\n\ndata = load_and_prepare(DATA_FILE, paraphrase_each_cq=True, paraphrases_per_cq=PARAPHRASES_PER_CQ, max_entries=MAX_ENTRIES)\n\n# Save to CSV and JSON\npd.DataFrame(data).to_csv(\"cqg_t5_training.csv\", index=False)\nwith open(\"cqg_t5_training.json\", \"w\") as f:\n    json.dump(data, f, indent=2)\n\nprint(f\"\\nSaved {len(data)} total examples (including original + paraphrased)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:59:33.134548Z","iopub.execute_input":"2025-06-01T14:59:33.134775Z","iopub.status.idle":"2025-06-01T15:10:19.347467Z","shell.execute_reply.started":"2025-06-01T14:59:33.134750Z","shell.execute_reply":"2025-06-01T15:10:19.346721Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n2025-06-01 14:59:39.510646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748789979.533732    5754 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748789979.540642    5754 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nSaved 1051 total examples (including original + paraphrased)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Manual Spot-Check of Paraphrased Critical Questions\n\nThis cell defines a function to quickly inspect sample paraphrased critical questions in the dataset, ensuring paraphrases are distinct and meaningful.\n\n### 4a. Functionality:  \n  The `manual_spot_check` function selects a specified number of paraphrased examples. For each example, it:\n  1. Extracts the original critical question and the generated triplet of questions.\n  2. Parses out the three questions.\n  3. Calculates string similarity between each question and the original.\n  4. Identifies and displays the paraphrased question most similar to the original.\n\n### 4b. Error Handling:  \n  If parsing fails, the function reports and skips the problematic entry.\n\n### 4c. Usage:  \n  Displays a side-by-side comparison for quick manual review.","metadata":{}},{"cell_type":"code","source":"import difflib\n\ndef manual_spot_check(data, n_pairs=5):\n    print(f\"\\n--- Manual Spot-Check of Paraphrased Data (showing first {n_pairs} examples) ---\\n\")\n    paraphrased_examples = [d for d in data if d.get(\"paraphrased_from\") is not None]\n    if not paraphrased_examples:\n        print(\"No paraphrased examples found.\")\n        return\n\n    samples = paraphrased_examples[:n_pairs]\n    for i, row in enumerate(samples, 1):\n        original_cq = row['paraphrased_from']\n        target_text = row['target_text']\n\n        try:\n            parts = target_text.split(\"1.\")[1].strip().split(\"2.\")\n            q1 = parts[0].strip()\n            q2_q3 = parts[1].split(\"3.\")\n            q2 = q2_q3[0].strip()\n            q3 = q2_q3[1].strip()\n            triplet = [q1, q2, q3]\n        except Exception as e:\n            print(f\"Error parsing triplet in pair {i}: {e}\")\n            continue\n\n        similarities = [difflib.SequenceMatcher(None, q, original_cq).ratio() for q in triplet]\n        most_similar_idx = similarities.index(max(similarities))\n        paraphrased_cq = triplet[most_similar_idx]\n\n        print(f\"Pair {i}:\")\n        print(\"Original CQ:    \", original_cq)\n        print(\"Paraphrased CQ: \", paraphrased_cq)\n        print(\"-\" * 80)\n\n# Usage\nmanual_spot_check(data, n_pairs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:10:19.349393Z","iopub.execute_input":"2025-06-01T15:10:19.349615Z","iopub.status.idle":"2025-06-01T15:10:19.365755Z","shell.execute_reply.started":"2025-06-01T15:10:19.349598Z","shell.execute_reply":"2025-06-01T15:10:19.365068Z"}},"outputs":[{"name":"stdout","text":"\n--- Manual Spot-Check of Paraphrased Data (showing first 5 examples) ---\n\nPair 1:\nOriginal CQ:     Are there alternative actions to working more closely with the USA's allies to achieve vacuuming up intelligence from Europe and the Middle East? If so, which is the most efficient action?\nParaphrased CQ:  Are there alternative actions to working more closely with the USA allies to vacuum up intelligence from Europe and the Middle East?\n--------------------------------------------------------------------------------\nPair 2:\nOriginal CQ:     What evidence does Clinton have that Iran had built \"covert facilities\" and stocked them with centrifuges? Are these claims supported by credible sources?\nParaphrased CQ:  What evidence does Clinton have that Iran had built \"covert facilities\" and stocked them with centrifuges?\n--------------------------------------------------------------------------------\nPair 3:\nOriginal CQ:     What is the evidence for the claim that NATO has only invoked Article 5 once, after 9/11? Is this a verifiable fact?\nParaphrased CQ:  What is the evidence for the claim that NATO has invoked Article 5 only once after 9/11?\n--------------------------------------------------------------------------------\nPair 4:\nOriginal CQ:     Are there alternative actions to USA sanctioning Iran to achieve Iran not having nuclear bombs? If so, which is the most efficient action?\nParaphrased CQ:  Are there alternative actions to sanction Iran by the USA to achieve Iran not having nuclear bombs?\n--------------------------------------------------------------------------------\nPair 5:\nOriginal CQ:     If Trump has a cavalier attitude toward nuclear weapons, might a nuclear war happen? What evidence supports this claim?\nParaphrased CQ:  If Trump has a cavalier attitude toward nuclear weapons, could a nuclear war happen?\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 5. Tokenization and Dataset Preparation for FLAN-T5\n\nThis section prepares the dataset for fine-tuning or evaluating the FLAN-T5 model on the Critical Question Generation (CQG) task. It ensures that the input and target sequences are properly tokenized and formatted for supervised training using Hugging Faceâ€™s `transformers` and `datasets` libraries.\n\n### 5a. Dataset Loading & Splitting\n\n- The CSV file is loaded using Hugging Faceâ€™s `load_dataset` function.\n- It is assumed that the CSV contains two columns: `\"input_text\"` and `\"target_text\"`.\n- The dataset is split into training and test sets, with 20% of the data reserved for testing.\n- A random seed is specified (`seed=42`) to ensure reproducibility.\n\n### 5b. Tokenizer Initialization\n\n- The `AutoTokenizer` is used to load the tokenizer for the `google/flan-t5-small` model.\n- This tokenizer is responsible for:\n  - Converting text inputs into token IDs.\n  - Adding special tokens required by the model.\n  - Handling truncation and padding.\n\n### 5c. Tokenization Function\n\n- A custom function is defined to tokenize both the input and target text.\n- For each example:\n  - `\"input_text\"` is tokenized with a maximum length of 512 tokens.\n  - `\"target_text\"` is tokenized with a maximum length of 128 tokens.\n  - Both are padded to fixed lengths (`padding=\"max_length\"`) and truncated if necessary.\n  - The targetâ€™s token IDs (`input_ids`) are stored in the `\"labels\"` field, which is expected by the model during training.\n\n### 5d. Batch Tokenization & Formatting\n\n- The dataset is tokenized in batches using the `.map()` method for efficiency.\n- The original columns (`\"input_text\"` and `\"target_text\"`) are removed to retain only the tokenized data.\n- The final dataset is formatted for PyTorch using `.set_format(\"torch\")`, making it ready for training with Hugging Faceâ€™s `Trainer` API.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"csv\", data_files=\"cqg_t5_training.csv\")[\"train\"]\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\nmax_input_length = 512\nmax_target_length = 128\n\ndef tokenize_function(example):\n    input_enc = tokenizer(\n        example[\"input_text\"],\n        max_length=max_input_length,\n        padding=\"max_length\",\n        truncation=True\n    )\n    target_enc = tokenizer(\n        example[\"target_text\"],\n        max_length=max_target_length,\n        padding=\"max_length\",\n        truncation=True\n    )\n    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n    return input_enc\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns([\"input_text\", \"target_text\"])\ntokenized_dataset.set_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:10:19.366546Z","iopub.execute_input":"2025-06-01T15:10:19.366810Z","iopub.status.idle":"2025-06-01T15:10:21.584676Z","shell.execute_reply.started":"2025-06-01T15:10:19.366792Z","shell.execute_reply":"2025-06-01T15:10:21.583969Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e81caeb30ab4eae8e8119de4ec851e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822c3e86652447a3b863847adb0abad0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e0b5c4fa0c4450b74014597d3fd1f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79aaacd055fb4fe8ad22d6540ee37ba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35880f0a06c14b5c8ee3cebabd15afa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/840 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f8bb2b9d2894cfba64b35b29468824b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/211 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3141a65298d84269934c232f1f889da7"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## 6. Custom Dataset Class for Evaluation\n\nThis cell defines a simple PyTorch Dataset, `CQGDataset`, for easy access to data samples and integration with PyTorch DataLoader.\n\n- `CQGDataset` wraps a data list, providing `__getitem__` and `__len__` methods.\n- An instance, `eval_dataset`, is created from the processed `data` list for evaluation or custom loading.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CQGDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    def __getitem__(self, idx):\n        return self.data[idx]\n    def __len__(self):\n        return len(self.data)\n\neval_dataset = CQGDataset(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:10:21.585415Z","iopub.execute_input":"2025-06-01T15:10:21.585625Z","iopub.status.idle":"2025-06-01T15:10:21.590822Z","shell.execute_reply.started":"2025-06-01T15:10:21.585607Z","shell.execute_reply":"2025-06-01T15:10:21.590093Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 7. Model Loading\n\nThis cell loads the FLAN-T5-small model and moves it to the appropriate device (GPU if available, otherwise CPU). This prepares the model for inference or further fine-tuning.","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_NAME = \"google/flan-t5-small\"\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:10:21.591759Z","iopub.execute_input":"2025-06-01T15:10:21.592030Z","iopub.status.idle":"2025-06-01T15:10:23.830962Z","shell.execute_reply.started":"2025-06-01T15:10:21.591999Z","shell.execute_reply":"2025-06-01T15:10:23.830387Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4553b1b381d4ccd82cd575c8740443c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a09002e8c1048a0912df1e4fba500cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eaf9762d41949e49280d4412d9aeb9d"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## 8. Fine-Tuning FLAN-T5\n\nThis section sets up and executes the fine-tuning process for the FLAN-T5 model using Hugging Faceâ€™s `Trainer` API. It defines training parameters, handles data collation, and initiates training with automatic evaluation and checkpointing.\n\n### 8a. Imports\n\n- Required classes are imported from the `transformers` library:\n  - `TrainingArguments`: For configuring training behavior.\n  - `DataCollatorForSeq2Seq`: For dynamic padding of input/output sequences.\n  - `Trainer`: For managing the training loop.\n\n### 8b. TrainingArguments\n\n- Training configuration is specified using `TrainingArguments`:\n  - `output_dir`: Directory to save training results.\n  - `per_device_train_batch_size` and `per_device_eval_batch_size`: Batch sizes for training and evaluation.\n  - `num_train_epochs`: Number of complete passes over the training data.\n  - `eval_strategy` and `save_strategy`: Set to `\"epoch\"` to evaluate and save after each epoch.\n  - `logging_dir` and `logging_steps`: Set up logging directory and frequency.\n  - `save_total_limit`: Keeps the most recent 2 checkpoints to limit storage use.\n  - `load_best_model_at_end`: Automatically reloads the best-performing model after training.\n  - `report_to`: Set to `\"none\"` to disable reporting to external services like Weights & Biases.\n\n### 8c. Data Collator\n\n- `DataCollatorForSeq2Seq` is used to:\n  - Dynamically pad inputs and labels to the maximum length within each batch.\n  - Ensure compatibility with sequence-to-sequence architectures like T5.\n\n### 8d. Trainer Setup and Training\n\n- The `Trainer` is initialized with:\n  - The model to be trained.\n  - Training arguments.\n  - Tokenized training and evaluation datasets.\n  - Tokenizer and data collator.\n- Training is started with `.train()`.\n- After training:\n  - The fine-tuned model is saved to the `\"finetuned_flant5_model\"` directory.\n  - The tokenizer is also saved for consistent downstream inference.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n\nprint(\"Fine-tuning Flan-T5...\")\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=20,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=base_model)\ntrainer = Trainer(\n    model=base_model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\ntrainer.train()\ntrainer.save_model(\"finetuned_flant5_model\")\ntokenizer.save_pretrained(\"finetuned_flant5_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:10:23.831801Z","iopub.execute_input":"2025-06-01T15:10:23.832083Z","iopub.status.idle":"2025-06-01T15:22:53.566426Z","shell.execute_reply.started":"2025-06-01T15:10:23.832052Z","shell.execute_reply":"2025-06-01T15:22:53.565804Z"}},"outputs":[{"name":"stdout","text":"Fine-tuning Flan-T5...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2100' max='2100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2100/2100 12:26, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.567800</td>\n      <td>2.615092</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.442700</td>\n      <td>1.897156</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.931100</td>\n      <td>1.583439</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.711300</td>\n      <td>1.461299</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.576400</td>\n      <td>1.402595</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.482900</td>\n      <td>1.367690</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.421400</td>\n      <td>1.342189</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.383100</td>\n      <td>1.321624</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.345500</td>\n      <td>1.304103</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.330100</td>\n      <td>1.294449</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.311400</td>\n      <td>1.281268</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.272700</td>\n      <td>1.272483</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.294600</td>\n      <td>1.267001</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.254600</td>\n      <td>1.259429</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.247300</td>\n      <td>1.256617</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.229300</td>\n      <td>1.252161</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.243900</td>\n      <td>1.250530</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.246900</td>\n      <td>1.247857</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.263000</td>\n      <td>1.247096</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.254100</td>\n      <td>1.246728</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('finetuned_flant5_model/tokenizer_config.json',\n 'finetuned_flant5_model/special_tokens_map.json',\n 'finetuned_flant5_model/spiece.model',\n 'finetuned_flant5_model/added_tokens.json',\n 'finetuned_flant5_model/tokenizer.json')"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 9. Visualizing Training and Validation Loss\n\nThis section visualizes the training and validation loss curves over epochs using Matplotlib. It helps in understanding the model's learning behavior and diagnosing overfitting or underfitting.\n\n### 9a. Extract Training Logs\n\n- After training completes, the `Trainer` stores logs in `trainer.state.log_history`.\n- Each log entry may contain:\n  - `loss`: Training loss at a given step or epoch.\n  - `eval_loss`: Evaluation (validation) loss at a given epoch.\n  - `step` and `epoch`: Step or epoch at which the log was recorded.\n\n### 9b. Parse Logs for Plotting\n\n- Iterate through `log_history` to collect:\n  - Training loss and corresponding epochs.\n  - Evaluation loss and corresponding epochs.\n- Separate lists are maintained for plotting each loss type.\n\n### 9c. Plot Loss Curves\n\n- Matplotlib is used to plot training and validation loss curves.\n- The X-axis represents epochs, and the Y-axis represents loss values.\n- Both curves are displayed in a single plot for easy comparison.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# After training with Hugging Face Trainer\n# Assuming you have run trainer.train()\nlog_history = trainer.state.log_history\n\n# Extract losses and steps/epochs\ntrain_loss = []\neval_loss = []\nsteps = []\nepochs = []\neval_steps = []\neval_epochs = []\n\nfor log in log_history:\n    if 'loss' in log:\n        train_loss.append(log['loss'])\n        steps.append(log.get('step', None))\n        epochs.append(log.get('epoch', None))\n    if 'eval_loss' in log:\n        eval_loss.append(log['eval_loss'])\n        eval_steps.append(log.get('step', None))\n        eval_epochs.append(log.get('epoch', None))\n\n# Plot Loss vs Epochs (if you want per-epoch, use unique epochs)\nplt.figure(figsize=(10,6))\nplt.plot(epochs, train_loss, label='Training Loss')\nplt.plot(eval_epochs, eval_loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss vs Epoch')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:22:53.567276Z","iopub.execute_input":"2025-06-01T15:22:53.568067Z","iopub.status.idle":"2025-06-01T15:22:53.769622Z","shell.execute_reply.started":"2025-06-01T15:22:53.568038Z","shell.execute_reply":"2025-06-01T15:22:53.768823Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## 10. Model Evaluation with Batched Inference\n\nThis section defines a function to evaluate the fine-tuned FLAN-T5 model using batched input data. It generates predictions with sampling-based decoding strategies and returns the generated outputs alongside reference texts and inputs.\n\n### 10a. Prepare Evaluation Dataset\n\n- Removes the `\"paraphrased_from\"` key from each example to keep only relevant fields (`\"input_text\"` and `\"target_text\"`).\n\n### 10b. Batched Evaluation Function\n\n- Uses a `DataLoader` to iterate over the dataset in batches.\n- Tokenizes input texts with padding and truncation.\n- Generates outputs with parameters enabling nucleus and top-k sampling, temperature scaling, repetition penalties, and early stopping.\n- Collects predictions, references, and input texts.\n- Optionally limits the number of evaluated samples via `sample_size`.","metadata":{}},{"cell_type":"code","source":"eval_dataset = [\n    {k: v for k, v in example.items() if k != \"paraphrased_from\"}\n    for example in data\n]\n\ndef evaluate_model_batched(model, tokenizer, dataset, batch_size=8, max_new_tokens=128, max_input_length=512, sample_size=None):\n    model.eval()\n    predictions, references, inputs_logged = [], [], []\n    dataloader = DataLoader(dataset, batch_size=batch_size)\n    count = 0\n    for batch in dataloader:\n        input_texts = batch[\"input_text\"]\n        target_texts = batch[\"target_text\"]\n        inputs = tokenizer(\n            input_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_input_length\n        ).to(model.device)\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                max_new_tokens=256,              \n                do_sample=True,           \n                top_k=30,                 \n                top_p=0.95,               \n                temperature=0.4,        \n                num_beams=3,              \n                early_stopping=True,\n                no_repeat_ngram_size=6,\n                repetition_penalty=1.4,\n                num_return_sequences=1,\n        )\n        predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        predictions.extend(predicted_texts)\n        references.extend(target_texts)\n        inputs_logged.extend(input_texts)\n        count += len(input_texts)\n        if sample_size and count >= sample_size:\n            break\n    return predictions[:sample_size], references[:sample_size], inputs_logged[:sample_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:22:53.771858Z","iopub.execute_input":"2025-06-01T15:22:53.772130Z","iopub.status.idle":"2025-06-01T15:22:53.780254Z","shell.execute_reply.started":"2025-06-01T15:22:53.772110Z","shell.execute_reply":"2025-06-01T15:22:53.779547Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 11. Evaluating the Fine-Tuned FLAN-T5 Model\n\nThis section demonstrates how to load the fine-tuned FLAN-T5 model and run batched evaluation on the prepared evaluation dataset. It then prints a sample prediction along with its input prompt and reference output.\n\n### 11a. Load Fine-Tuned Model\n\n- The fine-tuned model is loaded from the saved checkpoint directory `\"finetuned_flant5_model\"`.\n- The model is moved to the specified device (CPU or GPU).\n\n### 11b. Run Batched Evaluation\n\n- Calls the previously defined `evaluate_model_batched` function to generate predictions for a subset of the evaluation data (here, 100 samples).\n- Uses a batch size of 8 and sets the maximum tokens to generate per sample to 128.\n\n### 11c. Display Example Output\n\n- Prints the first example's:\n  - Input prompt.\n  - Generated output from the model.\n  - Ground truth reference text.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nprint(\"Evaluating fine-tuned Flan-T5...\")\nfinetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"finetuned_flant5_model\").to(device)\nfinetuned_predictions, finetuned_references, finetuned_inputs = evaluate_model_batched(\n    finetuned_model, tokenizer, eval_dataset, batch_size=8, max_new_tokens=128, sample_size=100\n)\n\nfor i in range(1):\n    print(f\"Example {i+1}\")\n    print(\"Input Prompt:\", finetuned_inputs[i])\n    print(\"\\nGenerated:\", finetuned_predictions[i])\n    print(\"\\nReference:\", finetuned_references[i])\n    print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:22:53.781123Z","iopub.execute_input":"2025-06-01T15:22:53.781661Z","iopub.status.idle":"2025-06-01T15:23:23.057886Z","shell.execute_reply.started":"2025-06-01T15:22:53.781641Z","shell.execute_reply":"2025-06-01T15:23:23.057201Z"}},"outputs":[{"name":"stdout","text":"Evaluating fine-tuned Flan-T5...\nExample 1\nInput Prompt: Given the following argumentation scheme and argument, generate exactly three distinct and critical questions. Each question should challenge the argument from a different angle. Present the questions in a numbered list format, starting from 1: Scheme: PracticalReasoning, Consequences, CauseToEffect, Example. Argument: CLINTON: \"which may prove to be an intelligence benefit we've got to do everything we can to vacuum up intelligence from Europe, from the Middle East That means we've got to work more closely with our allies, and that's something that Donald has been very dismissive of We're working with NATO, the longest military alliance in the history of the world, to really turn our attention to terrorism We're working with our friends in the Middle East, many of which, as you know, are Muslim majority nations Donald has consistently insulted Muslims abroad, Muslims at home, when we need to be cooperating with Muslim nations and with the American Muslim community They're on the front lines They can provide information to us that we might not get anywhere else They need to have close working cooperation with law enforcement in these communities, not be alienated and pushed away as some of Donald's rhetoric, unfortunately, has led to\" 1. 2. 3.\n\nGenerated: 1. What is the basis for Clinton's claim that NATO is the longest military alliance in the history of the world, and what evidence is there to support this claim? 2. How does Clinton plan to \" vacuum up\" intelligence from Europe, from the Middle East, and what are the potential consequences of working more closely with our allies? 3. How does Clinton plan to work more closely with NATO, and what specific actions does she propose to achieve this?\n\nReference: 1. Is the current political situation actually a typical case of other political situations that require working closely with NATO and our allies? How widely applicable is the generalization? 2. What evidence is there that Donald Trump's rhetoric has led to the alienation of Muslim communities, and how would Clinton's approach to working with these communities be more effective? 3. How does Clinton define \"working more closely\" with allies, and what specific actions or policies would she implement to achieve this goal?\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 12. Computing BERTScore for Fine-Tuned Model Outputs\n\nThis section defines a function to compute BERTScore, a semantic similarity metric between generated predictions and reference texts. It first cleans the texts by removing leading numbering or punctuation, then calculates and prints Precision, Recall, and F1 scores.\n\n### 12a. Cleaning Function\n\n- `clean_numbering(text)` removes digits, dots, and leading/trailing spaces from each word in the text.\n- This normalization helps to avoid skewing the BERTScore by extraneous numbering or formatting.\n\n### 12b. BERTScore Computation\n\n- The `compute_bertscore` function:\n  - Cleans predictions and references.\n  - Optionally limits evaluation to a sample size (default 100).\n  - Uses the `score` function from the `bertscore` library to compute Precision, Recall, and F1.\n  - Prints the averaged metric values.\n  \n### 12c. Usage\n\n- Calls `compute_bertscore` on the fine-tuned model's predictions and references to evaluate semantic quality.","metadata":{}},{"cell_type":"code","source":"from bert_score import score\n\ndef clean_numbering(text):\n    return \" \".join(word.strip(\"1234567890. \") for word in text.strip().split())\n\ndef compute_bertscore(preds, refs, sample_size=100):\n    clean_preds = [clean_numbering(p) for p in preds]\n    clean_refs  = [clean_numbering(r) for r in refs]\n    sample_preds = clean_preds[:sample_size]\n    sample_refs = clean_refs[:sample_size]\n    P, R, F1 = score(sample_preds, sample_refs, lang=\"en\", verbose=True)\n    print(\"\\n--- BERTScore Results ---\")\n    print(f\"Precision: {P.mean().item():.4f}\")\n    print(f\"Recall:    {R.mean().item():.4f}\")\n    print(f\"F1:        {F1.mean().item():.4f}\")\n\nprint(\"\\nFine-tuned Model BERTScore:\")\ncompute_bertscore(finetuned_predictions, finetuned_references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:23:23.058598Z","iopub.execute_input":"2025-06-01T15:23:23.058825Z","iopub.status.idle":"2025-06-01T15:23:27.833279Z","shell.execute_reply.started":"2025-06-01T15:23:23.058807Z","shell.execute_reply":"2025-06-01T15:23:27.832664Z"}},"outputs":[{"name":"stdout","text":"\nFine-tuned Model BERTScore:\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4268667f2bce469f84daf3951c7484c6"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b16471baa47471ab5c3581deb57b2bf"}},"metadata":{}},{"name":"stdout","text":"done in 2.31 seconds, 43.28 sentences/sec\n\n--- BERTScore Results ---\nPrecision: 0.8877\nRecall:    0.8837\nF1:        0.8856\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 13. Generating Critical Questions for a Single Input\n\nThis function generates exactly three distinct critical questions based on a provided argumentation scheme and argument. The questions challenge the argument from different perspectives and are formatted as a numbered list.\n\n### 13a. Function Description\n\n- **Inputs:**\n  - `model`: The fine-tuned FLAN-T5 model.\n  - `tokenizer`: The tokenizer associated with the model.\n  - `scheme_str`: A string describing the argumentation scheme.\n  - `argument`: The argument text to be challenged.\n\n- **Process:**\n  - Constructs a prompt combining the scheme and argument, instructing the model to generate three distinct critical questions.\n  - Tokenizes and encodes the input with padding, truncation, and a max length of 512 tokens.\n  - Generates output using beam search (`num_beams=3`), with sampling parameters set to encourage diversity and avoid repetition.\n  - Decodes the generated token IDs back to text.\n\n- **Output:**\n  - Returns the generated critical questions as a single string.","metadata":{}},{"cell_type":"code","source":"def generate_for_input(model, tokenizer, scheme_str, argument):\n    model.eval()\n    input_text = (\n        f\"Given the following argumentation scheme and argument, generate exactly three distinct and critical questions. \"\n        f\"Each question should challenge the argument from a different angle. Present the questions in a numbered list format, \"\n        f\"starting from 1: Scheme: {scheme_str}. Argument: {argument} 1. 2. 3.\"\n    )\n    input_ids = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=512\n    ).input_ids.to(model.device)\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids,\n            max_new_tokens=128,\n            do_sample=True,           \n            top_k=30,                 \n            top_p=0.95,               \n            temperature=0.4,        \n            num_beams=3,              \n            early_stopping=True,\n            no_repeat_ngram_size=6,\n            repetition_penalty=1.4,\n            num_return_sequences=1,\n        )\n    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:23:27.834093Z","iopub.execute_input":"2025-06-01T15:23:27.834282Z","iopub.status.idle":"2025-06-01T15:23:27.839631Z","shell.execute_reply.started":"2025-06-01T15:23:27.834268Z","shell.execute_reply":"2025-06-01T15:23:27.838947Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## 14. Generating and Extracting Critical Questions from Evaluation Data\n\nThis section loads a JSON evaluation dataset, generates critical questions for each argument using the fine-tuned model, extracts the questions from the raw model output, and saves the structured results to a JSON file.\n\n### 14a. Loading Evaluation Data\n\n- Loads evaluation data from a JSON file containing argument entries keyed by unique IDs.\n- Each entry contains the argument text (`\"intervention\"`) and associated argumentation schemes (`\"schemes\"`).\n\n### 14b. Extracting Critical Questions\n\n- Defines `extract_cqs_from_output` to parse generated text and extract critical questions formatted as a numbered list.\n- Uses regex to find numbered questions or falls back to splitting by numbers if fewer than three are found.\n- Returns the first three extracted questions as a list of dictionaries with key `\"cq\"`.\n\n### 14c. Generating Questions for Each Example\n\n- Iterates over each entry in the evaluation dataset.\n- Constructs a scheme string by joining unique schemes.\n- Cleans the argument text by removing newline characters.\n- Generates critical questions by calling `generate_for_input` with the fine-tuned model.\n- Extracts the questions from the generated output.\n- Stores the questions in a dictionary keyed by the example ID.\n\n### 14d. Saving Results\n\n- Writes the generated critical questions to a JSON file for further analysis or submission.","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nfrom tqdm.notebook import tqdm\n\n# Load evaluation data\nwith open(\"/kaggle/input/eval-data/eval_data.json\", \"r\", encoding=\"utf-8\") as f:\n    eval_data = json.load(f)\n\ndef extract_cqs_from_output(output_text):\n    \"\"\"\n    Extracts critical questions from a generated numbered list string.\n    Returns a list of dicts: [{\"cq\": question1}, {\"cq\": question2}, {\"cq\": question3}]\n    \"\"\"\n    # Try to find questions matching \"1. ... 2. ... 3.\"\n    matches = re.findall(r\"\\d+\\.\\s*([^\\n]+)\", output_text)\n    # If few found, fallback to splitting by numbers\n    if len(matches) < 3:\n        matches = [x.strip() for x in re.split(r\"\\d+\\.\\s*\", output_text) if x.strip()]\n    # Only keep first 3\n    return [{\"cq\": cq.strip()} for cq in matches[:3]]\n\n# --------- Generate for Fine-tuned Model ---------\noutputs_finetuned = {}\nfor k, entry in tqdm(eval_data.items(), desc=\"Generating (finetuned)\"):\n    schemes = list(set(entry.get(\"schemes\", [])))\n    scheme_str = \", \".join(schemes)\n    argument = entry[\"intervention\"].replace(\"\\n\", \" \").strip()\n    generated_text_fine = generate_for_input(finetuned_model, tokenizer, scheme_str, argument)\n    cqs = extract_cqs_from_output(generated_text_fine)\n    outputs_finetuned[k] = {\"cqs\": cqs}\n\nwith open(\"/kaggle/working/fine_tuned_model_generated_cqs.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(outputs_finetuned, f, indent=4, ensure_ascii=False)\nprint(\"Fine-tuned model output saved to /kaggle/working/fine_tuned_model_generated_cqs.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:29:41.182255Z","iopub.execute_input":"2025-06-01T15:29:41.182993Z","iopub.status.idle":"2025-06-01T15:29:49.783265Z","shell.execute_reply.started":"2025-06-01T15:29:41.182962Z","shell.execute_reply":"2025-06-01T15:29:49.782441Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating (finetuned):   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eaf60f6d483497690e51df291126210"}},"metadata":{}},{"name":"stdout","text":"Fine-tuned model output saved to /kaggle/working/fine_tuned_model_generated_cqs.json\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 15. Evaluation Script for Critical Question Generation\n\nThis evaluation script (`evaluate.py`) is used to quantitatively assess generated critical questions against reference questions using semantic similarity or BLEURT metrics.\n\n- It accepts command-line arguments for:\n  - `metric`: Choice between `'similarity'` (using Sentence-BERT) and `'bleurt'` metrics.\n  - `input_path`: Path to the reference dataset JSON file.\n  - `submission_path`: Path to the generated questions JSON file.\n  - `threshold`: Similarity score threshold to consider a match valid.\n\n- The script loads the evaluation models and data, then compares each generated question with reference questions.\n- It checks for repeated questions, calculates scores per intervention, and aggregates overall performance.\n- Results including labeled questions and evaluation scores are saved to a new JSON file.\n\n### 15a. Note:\n\nThis evaluation script is imported from the shared-task website for Critical Questions Generation: \nhttps://hitz-zentroa.github.io/shared-task-critical-questions-generation/","metadata":{}},{"cell_type":"code","source":"%%writefile evaluate.py\nimport json\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import Counter\nimport sys\nimport argparse\nimport logging\n# from evaluate import load # if you don't want to use bleurt and you don't have it installed, you can comment this, gpu might be needed\n\nlogger = logging.getLogger(__name__)\n\ndef main():\n    # arguments\n    parser = argparse.ArgumentParser(prog='Evaluate')\n    parser.add_argument('--metric', default='similarity', type=str, choices=['similarity', 'bleurt'])\n    parser.add_argument('--input_path', type=str, default='test.json', help='Path of the test set.')\n    parser.add_argument('--submission_path', type=str, default='output.json', help='Path where the generated questions have been saved.')\n    parser.add_argument('--threshold', type=float, default=0.6, help='Threshold to determine when the sentences are not similar. For bleurt, the threshold should probably be a negative number.') \n    args = parser.parse_args()\n\n    #logger\n    logging.basicConfig(filename='eval.log', level=logging.INFO)\n    logger.info('THRESHOLD: '+str(args.threshold)+'\\nMETRIC: '+args.metric)\n\n    # load the similarity model\n    if args.metric == 'similarity':\n        model = SentenceTransformer(\"stsb-mpnet-base-v2\") \n    elif args.metric == 'bleurt':\n        model = load(\"bleurt\", module_type=\"metric\")\n\n    # load the whole dataset\n    with open(args.input_path) as f:\n        reference=json.load(f)\n\n    with open(args.submission_path) as f:\n        new = json.load(f)\n\n    # start the evaluation\n    predicted_labels = []\n    punctuations = []\n\n    for instance in new.keys(): # for each intervention\n        punctuation = 0\n        reference_set = [ref['cq'] for ref in reference[instance]['cqs']]\n        if new[instance]['cqs'] != 'Missing CQs':\n            cqs_check = [cq['cq'] for cq in new[instance]['cqs']]\n            if len(cqs_check) != len(set(cqs_check)): # check the generated CQs are not repeated\n                logger.warning('There are repeated CQs in '+instance)\n            for i, line in enumerate(new[instance]['cqs']): # look into each question of the new cqs and find the most similar question in the references\n                winner = None\n                if args.metric == 'similarity':\n                    sentence_embedding = model.encode(line['cq'])\n                    reference_embedding = model.encode(reference_set)\n                    sims = model.similarity(sentence_embedding, reference_embedding).tolist()[0]\n                    \n                if args.metric == 'bleurt':\n                    results = model.compute(predictions=[line['cq']] * len(reference_set), references=reference_set)\n                    sims = results['scores']\n\n                winner = np.argmax(sims)\n                # make sure the similarity of the winning reference sentence is at least 0.6\n                if sims[winner] > args.threshold:\n                    label = reference[instance]['cqs'][winner]['label']\n                    if label == 'Useful':\n                        punctuation += 1/3\n                else: \n                    label = 'not_able_to_evaluate'\n                predicted_labels.append(label)\n                new[instance]['cqs'][i]['label'] = label\n        else:\n            # this should not happen if there are always 3 questions\n            predicted_labels.extend(['not_able_to_evaluate', 'not_able_to_evaluate', 'not_able_to_evaluate']) \n\n        punctuations.append(punctuation)\n\n    # metrics\n    print('Distribution of the labels:', Counter(predicted_labels))\n    print('Distribution of the intervention punctuation:', Counter(punctuations))\n    print('Overall punctuation', sum(punctuations)/len(punctuations))\n\n    # save the output\n    with open(args.submission_path[:-4]+'_eval_'+args.metric+'_'+str(args.threshold).replace('.', '')+'.json', 'w') as o:\n        json.dump(new, o, indent=4)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:23:35.913631Z","iopub.execute_input":"2025-06-01T15:23:35.914103Z","iopub.status.idle":"2025-06-01T15:23:35.920243Z","shell.execute_reply.started":"2025-06-01T15:23:35.914076Z","shell.execute_reply":"2025-06-01T15:23:35.919360Z"}},"outputs":[{"name":"stdout","text":"Overwriting evaluate.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 16. Evaluation of Fine-Tuned Model\n\nThis cell runs the evaluation script to assess the performance of the fine-tuned model.\n\n- **evaluate.py**: The evaluation script.\n- **--input_path**: Path to the test dataset in JSON format.\n- **--submission_path**: Path to the model's generated outputs for evaluation.\n- **--metric**: The metric used for evaluation (here, \"similarity\").\n- **--threshold**: Similarity threshold for considering predictions as correct.\n\nThe script compares the model-generated outputs against the ground truth in the test set and computes the chosen metric to determine the model's performance.","metadata":{}},{"cell_type":"code","source":"# Evaluation of Fine-Tuned Model\n\n!python evaluate.py \\\n  --input_path /kaggle/input/test-dataset/test.json \\\n  --submission_path /kaggle/working/fine_tuned_model_generated_cqs.json\\\n  --metric similarity \\\n  --threshold 0.60","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:29:54.359657Z","iopub.execute_input":"2025-06-01T15:29:54.360367Z","iopub.status.idle":"2025-06-01T15:30:09.010064Z","shell.execute_reply.started":"2025-06-01T15:29:54.360340Z","shell.execute_reply":"2025-06-01T15:30:09.009319Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-06-01 15:29:59.585828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748791799.607596   11171 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748791799.614264   11171 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.76it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.25it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 81.11it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.35it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 82.56it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.24it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 70.12it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.03it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 88.09it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.32it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 90.97it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.82it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 61.35it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.72it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 84.36it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.01it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 98.38it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.09it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 98.93it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.63it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 99.39it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.67it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 92.17it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.49it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 89.51it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.51it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 91.37it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.91it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 92.62it/s]\nBatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.45it/s]\nDistribution of the labels: Counter({'Useful': 8, 'Unhelpful': 4, 'Invalid': 2, 'not_able_to_evaluate': 1})\nDistribution of the intervention punctuation: Counter({0.3333333333333333: 3, 1.0: 1, 0.6666666666666666: 1})\nOverall punctuation 0.5333333333333333\n","output_type":"stream"}],"execution_count":20}]}